# Analysis Pipeline and Configuration

This document outlines the design for a `pipeline.py` script and a `config.py` file to orchestrate the molecular dynamics analysis workflow using the existing scripts in the `src/` directory.

## `config.py` Design

The `config.py` file will serve as a central place to define all parameters and input/output paths required by the analysis pipeline. This promotes modularity and avoids hardcoding values within the pipeline script.

It should include:

*   Input file paths (PSF, DCD, potentially initial Pt classification if not generated by the pipeline).
*   Output directory paths for intermediate and final results.
*   Parameters for each analysis step (e.g., `steps_per_frame` for `simulation_info.py`, `cutoff`, `r_max`, `nbins` for `classify_pt_atoms.py`, `min_res_time`, `max_off_time` for `residence_event_analysis.py`, distance `cutoff` for `filter_surface_fragments.py`).
*   Potentially, flags for enabling/disabling specific steps in the pipeline.

The configuration should be structured logically, perhaps using classes or nested dictionaries, to group related parameters. It should be designed to be easily imported and accessed by the `pipeline.py` script.

## `pipeline.py` Design

The `pipeline.py` script will orchestrate the execution of the individual analysis scripts in the correct order. It will read configuration from `config.py` and pass the necessary arguments to each script.

The pipeline should:

1.  Import the configuration from `config.py`.
2.  Define a function or class to represent the pipeline.
3.  Implement steps corresponding to each analysis script:
    *   Run `simulation_info.py`.
    *   Run `classify_pt_atoms.py`.
    *   Run `fragment_surface_analysis.py`.
    *   Run `filter_surface_fragments.py`.
    *   Run `residence_event_analysis.py`.
    *   Run `binding_metrics.py`.
4.  Ensure that the output of each step is correctly used as input for the subsequent step.
5.  Handle potential errors or exceptions during script execution.
6.  Provide command-line arguments for specifying the configuration file to use.
7.  Potentially, include options for running only a subset of the pipeline steps.

The execution of individual scripts within `pipeline.py` is achieved by dynamically constructing and running shell commands using Python's `subprocess` module, passing parameters from the `config.py` file as command-line arguments.
## Checkpointing for `fragment_surface_analysis.py`

The `fragment_surface_analysis.py` script includes built-in checkpointing functionality. It automatically detects if a partial output file (`surface_analysis.csv`) exists from a previous interrupted run. If a partial file is found, the script will automatically resume processing the molecular dynamics trajectory from the frame immediately following the last frame recorded in the existing file. This allows the pipeline to resume the most time-consuming step from where it left off without requiring specific handling or arguments from `pipeline.py` for resuming this particular script.

The logical flow of the pipeline will be:

`simulation_info.py` -> `classify_pt_atoms.py` -> `fragment_surface_analysis.py` -> `filter_surface_fragments.py` -> `residence_event_analysis.py` -> `binding_metrics.py`

Each script will take inputs and produce outputs that are then used by the next script in the sequence.