# Analysis Pipeline and Configuration

This document outlines the design for a `pipeline.py` script and a `config.py` file to orchestrate the molecular dynamics analysis workflow using the existing scripts in the `src/` directory.

## `config.py` Design

The `config.py` file will serve as a central place to define all parameters and input/output paths required by the analysis pipeline. This promotes modularity and avoids hardcoding values within the pipeline script.

It should include:

*   Input file paths (PSF, DCD, potentially initial Pt classification if not generated by the pipeline).
*   Output directory paths for intermediate and final results.
*   Parameters for each analysis step (e.g., `steps_per_frame` for `simulation_info.py`, `cutoff`, `r_max`, `nbins` for `classify_pt_atoms.py`, `min_res_time`, `max_off_time` for `residence_event_analysis.py`, distance `cutoff` for `filter_surface_fragments.py`).
*   Potentially, flags for enabling/disabling specific steps in the pipeline.

The configuration should be structured logically, perhaps using classes or nested dictionaries, to group related parameters. It should be designed to be easily imported and accessed by the `pipeline.py` script.

## `pipeline.py` Design

The `pipeline.py` script will orchestrate the execution of the individual analysis scripts in the correct order. It will read configuration from `config.py` and pass the necessary arguments to each script.

The pipeline should:

1.  Import the configuration from `config.py`.
2.  Define a function or class to represent the pipeline.
3.  Implement steps corresponding to each analysis script:
    *   Run `simulation_info.py`.
    *   Run `classify_pt_atoms.py`.
    *   Run `fragment_surface_analysis.py`.
    *   Run `filter_surface_fragments.py`.
    *   Run `residence_event_analysis.py`.
    *   Run `binding_metrics.py`.
4.  Ensure that the output of each step is correctly used as input for the subsequent step.
5.  Handle potential errors or exceptions during script execution.
6.  Provide command-line arguments for specifying the configuration file to use.
7.  Potentially, include options for running only a subset of the pipeline steps.

The execution of individual scripts within `pipeline.py` can be achieved using Python's `subprocess` module or by importing the scripts as modules and calling their main functions (if they are designed to be importable). Using `subprocess` might be simpler given the current script structure which uses `argparse`.

The logical flow of the pipeline will be:

`simulation_info.py` -> `classify_pt_atoms.py` -> `fragment_surface_analysis.py` -> `filter_surface_fragments.py` -> `residence_event_analysis.py` -> `binding_metrics.py`

Each script will take inputs and produce outputs that are then used by the next script in the sequence.